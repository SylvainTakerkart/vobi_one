# Author: Philippe Katz <philippe.katz@gmail.com>,
#         Flavien Garcia <flavien.garcia@free.fr>,
#         Sylvain Takerkart <Sylvain.Takerkart@univ-amu.fr>
# License: BSD Style.

try:
    from neuroProcesses import * # Provides a hierarchy to get object's path
except:
    print 'Impossible to import neuroProcesses'
try:
    import os
except ImportError:
    print 'Impossible to import os'

try:
    import shutil
except ImportError:
    print 'Impossible to import shutil'

try:	
    from soma import aims
except ImportError:
    print 'Impossible to import soma.aims'
    
try:
    import model_definition
except:
    try:
        import oidata.model_definition as model_definition
    except ImportError:
        print 'Impossible to import glm'

try:
    import numpy as np
except ImportError:
    print 'Impossible to import numpy'

try:
    import pickle
except ImportError:
    print 'Impossible to import pickle'

try:
    from neuroHierarchy import databases
except ImportError:
    print 'Impossible to import neuroHierarchy'
try:   
    from numpy import fft # Needed to recovery module of Fourier Transform
except ImportError:
    print 'Impossible to import fft from numpy' # Raises an exception
try:
    import matplotlib.pyplot as plt # Needed to plot data graph
except ImportError:
    print 'Impossible to import matplotlib.pyplot' # Raises an exception
try:
    from math import log # Needed to recovery the logarithm 
except ImportError:
    print 'Impossible to import log from math' # Raises an exception
try:
    from scipy.optimize import fmin,fmin_l_bfgs_b
except ImportError:
    print 'Impossible to import fmin,fmin_l_bfgs_b from scipy.optimize' # Raises an exception
    
class OiSession:
    """ Session-level class for optical imaging

    This class contains an ensemble of session-level functions for imaging processing and structured saving.
    It can be used for some of these functions in database mode or in manual mode :

    * Database mode

    1. Used by entering a valid path, it checks that the path is valid, if not it returns a warning and saves in the path given by the user.
    2. Used without any path, it saves in using the path build by the function.

    * Manual mode

    It saves in the path given by the user without giving any warning message.

    Attributes
    ----------
    database : str
        Database's path
    protocol : str
        Protocol's name
    subject : str
        Subject's name
    session : str
        Session directory. Given by the prefix "session_" and the session date.
    filename : str
        Averaged image's filename
    mask_name : str
        Mask's filename
    format : {'.nii','.nii.gz'}
        Saving format of images. It can be NIFTI-1 Image ('.nii') or gzip compressed NIFTI-1 Image ('.nii.gz')
    database_mode : bool
        The database mode
    path : str
        The path returned by the last save function
    param : str or tuple
        The list of parameters used to create the linear model
    L : int
        The number of regressors kept to apply the linear model
    X : numpy array
        The matrix of regressors
    mask : numpy array
        The mask used to average an image's region
    averaged_region : numpy vector
        The time serie returned by a region averaging
    averaged_img : numpy array
        The averaged image returned by a session averaging
    averaged_spectrum : numpy array
        The averaged image returned by a Fourier Transform
    averaged_img_list : numpy array list
        List of the averaged images returned by a session averaging
    spectrums_list : numpy array list
        List of the averaged spectrums
    spectrums_list : numpy array list
        List of the averaged spectrums
    d_values : float list
        List of d values issued of Durbin-Watson Test
    groups_list : numpy array list list
        List of groups of averaged trials
    mean_of_each_analysis : float list list
        List of signal mean of each trial
    nifti_header : dict
        The NIFTI-1 file's header
    xmin : int
        Top left-hand corner on x-axis
    ymin : int
        Top left-hand corner on y-axis
    xmax : int
        Bottom right-hand corner on x-axis
    ymax : int
        Bottom right-hand corner on y-axis
    period : float
        Period between two samples in [seconds]
    start : int
        Sample of stimulus apparition
    fig : matplotlib Figure
        Create active figure
    axes : matplotlib Figure
        Set the current axis limits 
    color : str
        Define color of the mean depending on model
    paths_list_list_for_averaging : str list
        List of paths list needed to average files
    residuals_list : numpy array list
        List of the residuals paths file
    betas_list : numpy array list
        List of the betas paths file    
    nb_rois : int
        Number of different ROIs (Needed for comparison_of_rois_process)
    nb_groups : int
        Number of different analyses (Needed for comparison_of_analysis_process)
    cond_group_list : int list list
        List of differents groups of conditions
    leg_names : str list
        List of legend names
    
    Methods
    -------
    __init__( ... )
        Initializes class' attributes
    load_mask( ... )
        Loads an existing mask
    load_averaged_img( ... )
        Loads an existing session averaged image
    set_nifti_header( ... )
        Set values to NIFTI-1 header
    set_model_parameters( ... )
        Loads parameters
    create_model_definition( ... )
        Creates the matrix of regressors
    copy_model_definition( ... )
        Copies files X.txt and param.npz, which contains the regressors and parameters, from the session-level analysis directory to the trial-level directories
    create_physio_params_file_func( ... )
        Copy the file containing the physiological parameters and
    create_trials_conds_file_func( ... )
        Creates a text file listing raw images by conditions
    create_filelist_for_averaging( ... )
        Creates the list of images paths which will be used for session averaging
    average_trials( ... )
        Averages a list of images paths
    check_and_load_corners( ... )
        Check if corners are available with the corresponding image
    create_rectangle_mask( ... )
        Creates a rectangle mask which defines the region to average
    average_region( ... )
        Averages a region of an image
    create_and_average_spectrum_datas( ... )
        Creates and averages a list of spectrums over an ROI
    get_spectrum()
        Returns the module of the Fourier Tranform of the raw signal
    detect_spikes( ... ):
        Method which try to detect frequencies of spikes
    create_spectral_analysis_graph()
        Plots the mean of all spectrums, annotated with spikes frequencies
    create_and_average_d_values()
        Creates and averages a list of residuals over an ROI
    durbin_watson_test( ... )
        Apply Durbin-Watson test
    create_durbin_watson_results_graph()
        Plots the durbin watson results in a histogram
    average_trials_on_rois( ... )
        Creates groups with the differents ROIs
    group_trials_by_roi( ... )
        Group trials by Region Of Interest        
    extract_mean_group( ... )
        Extract trials for an unique ROI               
    trace_comparison_of_rois_graph()
        Plots the comparison of rois graph
    delete_trials( ... ):
        Return trials list without higher and lower trial                
    trace_trials_variability_graph()
        Plots the trials variability        
    average_trials_of_each_analysis( ... )
        Creates groups with the differents analysis
    group_trials_by_analysis()
        Group all averaged trials by analysis and averages them
    trace_comparison_of_analysis_graph()
        Plots the comparison of rois graph
    estimate_tau_and_heartbeat_frequency( ... )
        Estimate the time constant 'tau' and the heartbeat frequency over an ROI
        It uses the condition file to average a list of spectrums, using their paths
    fit( ... )
        Use Nelder-Mead simplex direct search method to try to estimate tau
        and heartbeat frequency
    create_fit_results_graph( ... )
        Plots histograms of tau and fh
    save_data_graph( ... )
        Saves the data graph as an PNG image
    save_model_definition( ... )
        Saves X.txt in session_level (in vsdi_data)
    save_model_parameters( ... )
        Saves param.txt in session_level (in vsdi_data)
    save_averaged( ... )
        Saves the averaged images
    save_mask( ... )
        Saves the mask in vsdi_data
    save_averaged_region( ... )
        Saves the time serie
    update_database( ... )
        Register created files in brainvisa's database
    """

    def __init__( self,database=None,protocol=None,subject=None,session=None,param=None,format='.nii',database_mode=False ,period=None):
        """Initializes class' attributes

        Parameters
        ----------
        database : str, optional
            Database's path
        protocol : str, optional
            Protocol's name
        subject : str, optional
            Subject's name
        session : str, optional
            Session directory. Given by the prefix "session_" and the session date.
        param : str or tuple, optional
            The list of parameters used to create the linear model
        format : {'.nii','.nii.gz'}, optional
            Saving format of images. It can be NIFTI-1 Image ('.nii') or gzip compressed NIFTI-1 Image ('.nii.gz')
        database_mode : bool, optional
            The database mode
        """
        self.database=database
        self.protocol=protocol
        self.subject=subject
        self.session=session
        self.filename=None
        self.mask_name=None
        self.database_mode=database_mode
        self.__setformat(format)
        self.set_model_parameters(param)
        self.L=None
        self.X=None
        self.mask=None
        self.averaged_region=None
        self.averaged_img=None
        self.averaged_img_list=None
        self.spectrums_list=None
        self.betas_list=None
        self.residuals_list=None
        self.path=None
        self.set_nifti_header() # Set values to NIFTI-1 header
        self.d_values=None
        self.period=period
        self.start=0
        
    def load_mask( self,filename=None,path=None ):
        """Loads an existing mask

        Parameters
        ----------
        filename : str, optional
            The name of mask's file
        path : str, optional
            The path of the mask

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if the mask really exist, -1 if the mask doesn't exist
        path : str
            The path of the mask
        mask : numpy array
            The mask used to average an image's region

        Raises
        ------
        ValueError
            If no file is selected or if the file does not exist
        """
        warning=0
        if path==None:
            if filename == None:
                raise ValueError("No file selected") # Raises an exception
            else:
                path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis','mask',filename) # Path creation
                self.mask_name=os.path.splitext(os.path.splitext(os.path.basename(filename))[0])[0] # Mask filename creation
        if os.path.lexists(path)==False: # If path does not exists
            warning=-1 # Returns a warning if the path does not correspond to the path created by the database mode
            raise ValueError("File does not exists") # Raises an exception
        
        Vol = aims.read(path.encode("utf8")) # Opening NIFTI file
        self.mask=np.transpose(np.array( Vol,copy=True )) # Image recuperation
        self.mask_name=os.path.splitext(os.path.splitext(os.path.basename(path))[0])[0]
        return (warning,path,self.mask)

    def load_averaged_img( self,analysis=None,filename=None,path=None ):
        """Loads an existing session averaged image

        Parameters
        ----------
        analysis : str
            The name of the analysis directory
        filename : str
            Averaged image's filename
        path : str
            Averaged image's path

        Returns
        -------
        path : str
            Averaged image's path
        averaged_img : numpy array
            Averaged image

        Raises
        ------
        ValueError
            If the file does not exist
        """
        if path==None:
            if analysis == None or filename == None:
                raise ValueError("No file selected") # Raises an exception
            else:
                if os.path.splitext(filename)[0][-4:]=='mean':
                    path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis',analysis,filename) # Path creation
                else:
                    exp='exp'+os.path.splitext(filename)[0][9:11]
                    trial='trial'+os.path.splitext(filename)[0][13:17]
                    path=os.path.join(self.database,self.protocol,self.subject,self.session,'oitrials_analysis',exp,trial,analysis,filename) # Path creation
                self.filename=os.path.splitext(filename)[0] # Image filename creation
        else:
            self.filename=os.path.splitext(os.path.basename(path))[0]

        if os.path.lexists(path)==False: # If path does not exists
            raise ValueError("File does not exists") # Raises an exception
        
        fname = path.encode("utf8")
        Vol = aims.read(fname) # Opening NIFTI file
        # NIFTI header recuperation
        self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
        self.nifti_header[ 'time_units' ] = Vol.header()[ 'time_units' ]
        self.nifti_header[ 'voxel_size' ] = Vol.header()[ 'voxel_size' ]
        if self.period is None:
            self.period = self.nifti_header['voxel_size'][3]
        self.averaged_img=np.transpose(np.array( Vol,copy=True )) # Image recuperation

        return (path,self.averaged_img)

    def set_nifti_header( self, period=1.0,time_units=8,xyz_units=2 ):
        """Set values to NIFTI-1 header

        Parameters
        ----------
        period : float, optional
            The acquisition period
        time_units : int, optional
            The time units
        xyz_units : int, optional
            The spatial units
        """
        self.nifti_header={'xyz_units':2, 'time_units':8, 'voxel_size':[1,1,1,period]}

    def set_model_parameters( self,param ):
        """Loads parameters

        Parameters
        ----------
        param : str or tuple
            The parameters used to create the Linear Model.
            It can be : a str, in case of the parameters are loaded from an existing file, param is the path of this file, or a tuple, in case of the parameters are set manually.
              param is a tuple which contains :
              0. The sampling frequency,
              1. The trial duration,
              2. The time constant of dye bleaching,
              3. The frequencies of environmental and physiological noises (a list),
              4. The fourier orders of environmental and physiological noises (a list, must have the same lenght as point 3),
              5. The number of regressors kept to create the Linear Model,
              6. The time-range parameters (minima),
              7. The time-range parameters (maxima, must have the same lenght as point 6)
        """
        if type(param) is tuple:
            self.param = param
            self.L=self.param[5]

        elif type(param) is str: # If param is a path
            f=open(param,'r') # Opening parameters file
            self.param=pickle.load(param)
            self.L=self.param[5]
            f.close() # Closing parameters file

            
    def create_model_definition( self , pathX ):
        """Creates the matrix of regressors and save a file to plot estimated
        shapes responses (development version so that the model's length can exceed the length of a trial
        
        Parameters
        ----------
        pathX : str
            Path of the GLM matrix
        """
        # Time vector
        # compute maximum length of a model response according to the given model parameters
        total_model_length = np.array(eval(self.param[7]))[0:6].sum()
        if total_model_length < self.param[1]:
            total_model_length = self.param[1]
        time_vector=np.arange(0,round(total_model_length*self.param[0])) / self.param[0]
        #time_vector=np.arange(0,round(self.param[1]*self.param[0])) / self.param[0]

        # X0: constant regressor
        X0 = np.ones([len(time_vector),1])

        # X1: Environmental noise from phantom experiment and physiological noise from blank trials
        X1 = model_definition.mod_bruits(eval(self.param[4]),eval(self.param[3]),time_vector)
 
        if self.param[2] != 0:
          # X2: Dye Bleaching (only included if the parameter is non null, ie. we do not want such
          # a regressor for intrinsic data
          X2 = model_definition.dye_bleaching(self.param[2],time_vector)
          

        # X3: Response Components
        alpha_range=model_definition.range_alpha(eval(self.param[6]),eval(self.param[7]),time_vector)

        if np.isscalar(alpha_range[0]):
            # then all ranges are zero-lengths, the user wants to use a fixed shape
            alpha = np.zeros([1,len(alpha_range)])
            alpha[:] = alpha_range
            r_alpha=model_definition.mod_response(alpha,time_vector)[0]
            r_alpha_light = r_alpha
            X3 = r_alpha

        else:
            # perform the full thing to create all the example shapes and the PCA
            alpha = alpha_range[0]
            for a in range(len(alpha_range)-1):
                alpha = model_definition.concat_alpha(alpha,alpha_range[a+1]) # Concatenation of the submatrix

            r_alpha=model_definition.mod_response(alpha,time_vector)[0] # Example-shapes creation

            ########## Envelop steps ##########
            # r_alpha = all generated example shapes
            # r_alpha_light = only a subset for plotting representative shapes

            alpha_range_light=model_definition.range_alpha_light(alpha_range,time_vector) # Light example-shapes creation

            alpha_light = alpha_range_light[0] # Initialization with first submatrix
            for a in range(len(alpha_range_light)-1): # For each submatrix
                alpha_light = model_definition.concat_alpha(alpha_light,alpha_range_light[a+1]) # Light concatenation of the submatrix   
            r_alpha_light,invalid_light,invalid_light_id=model_definition.mod_response(alpha_light,time_vector) # Light example-shapes creation
            nb_invalid=len(invalid_light) # Number of responses which exceed duration

            # Treatment 
            step=2*np.ones(len(alpha_range)-1) # Current step
            iter=0 # Current iteration
            maxiter=50 # Maximum of iteration to try to apply treatment
            while nb_invalid>0 and iter<maxiter: # Treatment to reduce the differents times of invalid response
                j=0 # First bad shape in invalid list
                for i in invalid_light_id: # For each bad shape in shapes list
                    pos=model_definition.getMax(invalid_light[j]-np.array(eval(self.param[6]))) # Gets parameter which has the maximum value between min and max
                    alpha_light[i][pos]=alpha_range[pos][len(alpha_range[pos])-step[pos]] # Change alpha by a smaller alpha
                j+=1 # Next bad shape
                r_alpha_light,invalid_light,invalid_light_id=model_definition.mod_response(alpha_light,time_vector) # Light example-shapes creation again
                nb_invalid=len(invalid_light) # Number of responses which still exceed duration
                step[pos]+=1 # Increase current step of parameter pos

            if os.path.lexists(os.path.split(pathX)[0]) == False: # If pathX does not exist
                os.makedirs(os.path.split(pathX)[0]) # Creation of path

            ###################################

            X3=np.linalg.svd(r_alpha,full_matrices=False) # Eigenvectors
            X3=X3[0][:,0:self.L] # Keeping of the first L eigenvectors

        #print X0.shape, X1.shape, X2.shape, X3.shape


        if self.param[2] != 0:
            # X: Model design
            #self.X=model_definition.concat_X(X1,X2,X3,time_vector) # Concatenation of X0 (=1), X1, X2, X3
            self.X = np.concatenate( (X0, X1, X2, X3), 1)
        else:
            # no bleaching for intrinsic data!
            #self.X=model_definition.concat_X(X1,X3,time_vector) # Concatenation of X0 (=1), X1, X3
            self.X = np.concatenate( (X0, X1, X3), 1)

        #print self.X.shape

        # Restrict the model definition to the actual length of a trial
        restricted_time_vector=np.arange(0,round(self.param[1]*self.param[0])) / self.param[0]
        nbr_samples_per_trial = len(restricted_time_vector)
        self.X = self.X[0:nbr_samples_per_trial,:]

        #print self.X.shape

        f=open(os.path.join(os.path.split(pathX)[0],'r_alpha_light').encode("utf8"),'wb') # Open file including example-shapes response file
        pickle.dump(r_alpha_light[0:nbr_samples_per_trial,:],f) # Save example-shapes response file
        f.close() # Closes file including example-shapes response file
        
    def copy_model_definition( self,analysis ):
        """Copies files X.txt and param.npz, which contains the regressors and parameters, from the session-level analysis directory to the trial-level directories

        Parameters
        ----------
        analysis : str
            The name of the analysis directory

        Raises
        ------
        ValueError
            If the file does not exist
        """
        path=os.path.join(self.database,self.protocol,self.subject,self.session) # Path creation

        if os.path.lexists(os.path.join(path,'oisession_analysis',analysis,'glm.txt')) == False or os.path.lexists(os.path.join(path,'oisession_analysis',analysis,'param.npz'))==False: # If param.npz and X.txt don't exist
            print os.path.join(path,'oisession_analysis',analysis,'glm.txt')
            print os.path.join(path,'oisession_analysis',analysis,'param.npz')
            raise ValueError( 'path does not exist') # Raises an exception

        experiences=os.listdir(os.path.join(path,'vsdi_data')) # Creation of a list of experiences directories

        for exp in experiences: # In each experience
            if os.path.isdir(os.path.join(path,'vsdi_data',exp)):
                trials=os.listdir(os.path.join(path,'vsdi_data',exp)) # Creation of a list of trials directories
                for trial in trials: # In each trial
                    if os.path.lexists(os.path.join(path,'vsdi_data',exp,trial,analysis))==False: # If the path designed by __pathAnaysis does not exist

                        os.makedirs(os.path.join(path,'vsdi_data',exp,trial,analysis)) # Creation of __pathAnalysis directory

                    # Copying the model definition file and the model parameters from session-level to trial-level
                    shutil.copy(os.path.join(path,'oisession_analysis',analysis,'glm.txt'),os.path.join(path,'vsdi_data',exp,trial,analysis))
                    shutil.copy(os.path.join(path,'oisession_analysis',analysis,'param.npz'),os.path.join(path,'vsdi_data',exp,trial,analysis))

    def create_physio_params_file_func( self, unimported_physio_file_path ):
        """Copy the file containing the physiological parameters and
        import it into the brainvisa database

        Parameters
        ----------
        unimported_physio_file_path : str
            The filename of the external file containing the physiological parameters

        Returns
        -------
        imported_physio_file_path : str
            The path to the file containing physiological parameters (frequency and phase of Hb and resp)
            as imported into the brainvisa database

        Raises
        ------
        ValueError
            If the file does not exist
        """
        path=os.path.join(self.database,self.protocol,self.subject,self.session) # Path creation

        if os.path.lexists(path) == False: # If path does not exist
            raise ValueError( 'path does not exist') # Raises an exception

        # creation of the path for the imported file containing physio params
        imported_physio_file_path=os.path.join(path,'oitrials_analysis','physio_params.txt')
        
        # copy the unimported file into its database location and name
        shutil.copy(unimported_physio_file_path, imported_physio_file_path)

        return imported_physio_file_path


    def create_trials_conds_file_func( self ):
        """Creates a text file listing raw images by conditions

        Returns
        -------
        path_cond : str
            The path to the condition file

        Raises
        ------
        ValueError
            If the file does not exist
        """
        path=os.path.join(self.database,self.protocol,self.subject,self.session) # Path creation

        if os.path.lexists(path) == False: # If path does not exist
            raise ValueError( 'path does not exist') # Raises an exception

        f=open(os.path.join(path,'oitrials_analysis','conditions.txt'),'w') # Opening condition file

        experiences=os.listdir(os.path.join(path,'oitrials_analysis')) # Creation of a list of experiences directories

        for exp in experiences: # In each experience
            if os.path.isdir(os.path.join(path,'oitrials_analysis',exp)):
                trials=os.listdir(os.path.join(path,'oitrials_analysis',exp)) # Creation of a list of trials directories
                for trial in trials: # In each trial
                    contenu=os.listdir(os.path.join(path,'oitrials_analysis',exp,trial,'raw')) # Creation of a list of data files
                    for name in contenu: # For each file
                        if os.path.splitext(os.path.join(path,'oitrials_analysis',exp,trial,'raw',name))[1]=='.nii' or (os.path.splitext(os.path.splitext(os.path.join(path,'oitrials_analysis',exp,trial,name))[0])[1]+os.path.splitext(os.path.join(path,'oitrials_analysis',exp,trial,'raw',name))[1])=='.nii.gz': # If file is a NIFTI-1 image or a gz compressed NIFTI-1 image
                            f.writelines((name,'\t',exp,'\t',trial,'\t',name[18:22],'\t','1','\n')) # Writes a line in condition file contaning the following columns :
                                                                                                    # filename
                                                                                                    # experience
                                                                                                    # trial
                                                                                                    # condition
                                                                                                    # selected (if file selected for processing or not (default '1' : selected)

        f.close() # Closing file

        path_cond=os.path.join(path,'oitrials_analysis','conditions.txt') # Condition file path creation

        return path_cond
        

    def create_filelist_for_averaging( self, analysis, condition_list, blank_based_suffix='' ):
        """Creates the list of images paths which will be used for session averaging

        Parameters
        ----------
        analysis : str
            The name of the analysis directory
        condition_list : list
            The list of conditions
        blank_based_suffix : str, optional
            The suffix of images' filename, used to create the list of images for averaging

        Raises
        ------
        ValueError
            If the file does not exist,
            If analysis is different of 'raw', glm_based', 'blank_based',
            If extension of files are differents of '.nii', '.nii.gz'
        """
        path=os.path.join(self.database,self.protocol,self.subject,self.session) # Path creation
        path_cond=os.path.join(path,'oitrials_analysis','conditions.txt') # Condition file path creation
        if os.path.lexists(path_cond) == False: # If file does not exist
            raise ValueError('No such file or directory: %s' % path_cond) # Raises an exception

        raw_name,experiences,trials,conditions,selected=np.loadtxt(path_cond, delimiter='\t', unpack=True,dtype=str) # Extraction of condition file values

        #print raw_name
        
        if type(condition_list)==list: # Singleton case
            condition_list=[condition_list]
            
        self.paths_list_list_for_averaging=[]
        for sl in range(len(condition_list)): # For each sublist
            paths_list_for_averaging = [] # List initialization
            for i in range(len(raw_name)): # For each name
                if list(condition_list[sl]).count(int(conditions[i][1:])) and int(selected[i]): # If file condition corresponds to the wanted condition and if file is selected
                    filename=os.path.splitext(os.path.splitext(raw_name[i])[0])[0] # Recuperation of filename

                    if analysis[0:3]=='raw': # If it is a raw image
                        path_trial=os.path.join(path,'oitrials_analysis',experiences[i],trials[i],analysis,filename) # Path creation

                    elif analysis[0:9]=='glm_based': # If it is a glm-based image
                        if blank_based_suffix == '': # Selection of denoised image
                            blank_based_suffix='_denoised' # Adding a suffix '_denoised'

                        path_trial=os.path.join(path,'oitrials_analysis',experiences[i],trials[i],analysis,filename+blank_based_suffix)  # Path creation

                    elif analysis[0:11]=='blank_based': # If it is a blank-based image
                        if blank_based_suffix == '': # Selection of denoised image
                            blank_based_suffix='_f0d_bks_d' # Adding a suffix '_f0d_bks_d'

                        path_trial=os.path.join(path,'oitrials_analysis',experiences[i],trials[i],analysis,filename+blank_based_suffix) # Path creation
                        
                    else:
                        raise ValueError("Analysis name has not a valid value: %s" % analysis) # Raises an exception
                    
                    if os.path.lexists(path_trial + '.nii'): # If selected file is a NIFTI-1 image
                        paths_list_for_averaging.append(path_trial + '.nii') # Adding '.nii' extension and recuperation in file list
                    elif os.path.lexists(path_trial + '.nii.gz'): # If selected file is a gz compressed NIFTI-1 image
                        paths_list_for_averaging.append(path_trial + '.nii.gz') # Adding '.nii.gz' extension and recuperation in file list
                    else:
                        raise ValueError('File does not exist, or extension not supported for: %s' % path_trial) # Raises an exception
            self.paths_list_list_for_averaging.append(paths_list_for_averaging)      
        self.cond_group_list = condition_list


    def average_trials( self, list = None ):
        """Averages a list of images

        It uses the condition file to average a list of image, using their paths

        Parameters
        ----------
        list : list, optional
            The list of images' paths which has to be averaged

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if list is not empty, -1 if list is empty
        """
        warning = 0
        
        if list == None:
            list = self.paths_list_list_for_averaging

        if list == []:
            warning = -1
            return warning  # Returns a warning list is empty

        self.averaged_img_list=[]
        for sublist in list:
            # Recuperation of image header and size, use the first image in the sublist
            fname = sublist[0].encode("utf8")
            Vol=aims.read(fname) # Opening NIFTI file
            # NIFTI header recuperation
            self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
            self.nifti_header[ 'time_units' ] = Vol.header()[ 'time_units' ]
            self.nifti_header[ 'voxel_size' ] = Vol.header()[ 'voxel_size' ]
            if self.period is None:
                self.period = self.nifti_header['voxel_size'][3]
            img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            # Recuperation of image's size
            tsize=img.shape[0]
            zsize=img.shape[1]
            ysize=img.shape[2]
            xsize=img.shape[3]

            self.averaged_img=np.zeros([tsize,zsize,ysize,xsize]) # Averaged image initialization

            # Images averaging
            j=0
            while j<len(sublist): # For each file in the sublist
            
                Vol = aims.read(sublist[j].encode("utf8")) # Opening NIFTI file
                img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            
                self.averaged_img=self.averaged_img+img # Adds image to averaged_img
                j=j+1
            
            self.averaged_img=self.averaged_img / len(sublist) # Divide by the length of the list
            self.averaged_img_list.append(self.averaged_img)
            
        return warning
        
    def check_and_load_corners( self, corner0, corner1, image):
        """Check if corners are available with the corresponding image
        and update top left-hand and bottom right-hand corners values on x-axis
        and y-axis
        
        Parameters
        ----------
        corner0 : int tuple
            The position of the top left-hand corner of the mask (x,y)
        corner1 : int tuple
            The position of the bottom right-hand corner of the mask (x,y)
        image : numpy array
            4-D float array data
            
        Raises
        ------
        ValueError
            If top left-hand corner value on x-axis is not correct
            If top left-hand corner value on y-axis is not correct
            If bottom right-hand corner value on x-axis is not correct
            If bottom right-hand corner value on y-axis is not correct
        """
        self.xmin=0 # Top left-hand corner value on x-axis 
        self.ymin=0 # Top left-hand corner value on y-axis
        self.xmax=image.shape[3] # Bottom right-hand corner value on x-axis
        self.ymax=image.shape[2] # Bottom right-hand corner value on y-axis
        if corner0!=None: # Top left-hand corner modified
            if corner0[0] in range(image.shape[3]): # If xmin is available
                self.xmin=corner0[0] # Recovery of xmin
            else:
                raise ValueError('Top left-hand corner value on x-axis must be between 0 and '+str(image.shape[3]-1))
            if corner0[1] in range(image.shape[2]): # If ymin is available
                self.ymin=corner0[1] # Recovery of ymin  
            else:
                raise ValueError('Top left-hand corner value on y-axis must be between 0 and '+str(image.shape[2]-1))
        if corner1!=None: # Bottom right-hand corner modified
            if corner1[0] in range(image.shape[3]+1) and corner1[0]>self.xmin: # If xmax is available
                self.xmax=corner1[0] # Recovery of xmax
            else:
                raise ValueError('Bottom right-hand corner value on x-axis must be between '+str(self.xmin)+' and '+str(image.shape[3]))
            if corner1[1] in range(image.shape[2]+1) and corner1[1]>self.ymin: # If ymax is available
                self.ymax=corner1[1] # Recovery of ymax 
            else:
                raise ValueError('Bottom right-hand corner value on y-axis must be between '+str(self.ymin)+' and '+str(image.shape[2]))        
    
    def create_rectangle_mask( self, corner0, corner1, image=None ):
        """Creates a rectangle mask which defines the region to average

        Parameters
        ----------
        corner0 : int tuple
            The position of the top left-hand corner of the mask (x,y)
        corner1 : int tuple
            The position of the bottom right-hand corner of the mask (x,y)
        image : numpy array, optional
            4-D float array data

        Returns
        -------
        mask : numpy array
            The mask used to average an image's region

        Raises
        ------
        ValueError
            If there is no image to average
        """

        # Image size recuperation
        if image == None: # If no image selected, try to use the averaged image
            if self.averaged_img != None:
                image=self.averaged_img
            else:
                raise ValueError("No image to process") # Raises an exception

        # Recuperation of image's size
        tsize=image.shape[0]
        zsize=image.shape[1]
        ysize=image.shape[2]
        xsize=image.shape[3]

        self.mask=np.zeros((zsize,ysize,xsize)) # Mask initialization
        
        self.check_and_load_corners(corner0,corner1,image) # Check if corners are available with image's shape 
        
        self.mask[:,self.ymin:self.ymax,self.xmin:self.xmax]=1 # Set to 1 the region of interest
        self.mask_name = 'mask'+'_'+str(self.xmin)+'_'+str(self.ymin)+'_'+str(self.xmax)+'_'+str(self.ymax) # Mask's filename creation
        return self.mask
        
    def average_region( self ):
        """Averages a region of an image

        Returns
        -------
        averaged_region : numpy vector
            The time serie resulting from the averaging of a region of interest
            
        Raises
        ------
        ValueError
            If the shapes of the mask and the raw image are not the same
        """
        try: # Apply mask on the image to select ROI
            roi=self.averaged_img*self.mask # Multiplies averaged image and mask for region of interest recovery
        except ValueError: # If shapes are not the same
            raise ValueError('The shapes of the mask ('+str(self.mask.shape)\
                       +') and the raw image ('+str(self.averaged_img.shape)+') are not the same => Mask can\'t be apply')
                       
        
        nbr_of_voxels = self.mask.sum()
        self.averaged_region = roi.sum(axis=2).sum(axis=2) / nbr_of_voxels
        return self.averaged_region

    def create_and_average_spectrum_datas(self,list=None):
        """Creates and averages a list of spectrums over an ROI
        It uses the condition file to average a list of spectrums, using their paths

        Parameters
        ----------
        list : list, optional
            The list of images' paths which has to be averaged

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if list is not empty, -1 if list is empty
        """
        warning = 0
        
        if list == None:
            list = self.spectrums_list

        if list == []:
            warning = -1
            return warning  # Returns a warning list is empty
            
        # Recover spectrum list
        # Recuperation of image header and size, use the first image in the sublist
        fname = list[0].encode("utf8")
        Vol=aims.read(fname) # Opening NIFTI file
        # NIFTI header recuperation
        self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
        self.nifti_header['time_units'] = Vol.header()[ 'time_units' ]
        self.nifti_header['voxel_size'] = Vol.header()[ 'voxel_size' ]
        if self.period is None:
            self.period = self.nifti_header['voxel_size'][3]
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            
        # Recuperation of image's size
        tsize=img.shape[0]
            
        self.averaged_spectrum=np.zeros([tsize]) # Averaged image initialization
            
        # Images averaging
        j=0
        while j<len(list): # For each file in the sublist
            Vol = aims.read(list[j].encode("utf8")) # Opening NIFTI file
            img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            self.averaged_img=img
            spectrum=np.reshape(self.get_spectrum(),(tsize))
            self.averaged_spectrum=self.averaged_spectrum+spectrum # Adds image to averaged_img
            j=j+1
                
        self.averaged_sectrum=self.averaged_spectrum / len(list) # Divide by the length of the list
            
        return warning

    def get_spectrum(self):
        """Returns the module of the Fourier Tranform of the raw signal over a ROI
        
        Returns
        -------
        spectrum : numpy 1-D array
            Module of the Fourier Transform on a selected ROI
        """
        av=self.average_region() # Average over a ROI
        y=np.reshape(av,(len(av))) #
        spectrum=np.abs(fft.fft(y)) # Calculates the module of the FT
        return spectrum
        
    def detect_spikes(self,averaged_spectrum_log,x):
        """Method which try to detect frequencies of spikes
        
        Parameters
        ----------
        averaged_spectrum_log : numpy 1-D array
            Logarithm of the averaged module of the Fourier Transform
        x : numpy 1-D array
            Time interval (x-axis)
        
        Returns
        -------
        f_list : float list
            Frequencies of the spikes
        y_list : float list
            Values on the y-axis for each frequency
        """
        tol=4. # Coefficient of tolerance to detect spikes
        f_list=[] # Spikes frequencies list initialization
        y_list=[] # Value for these frequencies
        d=1. # Value needed for the first step
        for i in range(1,len(averaged_spectrum_log)/2): # For each frequency
            spike=False # True if there is a good spike
            d_prev=d # Previous coeff
            d=averaged_spectrum_log[i]-averaged_spectrum_log[i-1] # Current coeff
            d_next=averaged_spectrum_log[i+1]-averaged_spectrum_log[i] # Next coeff
            if d>0 and d_prev<0 and d_next<0: # Spike with coeff positif
                spike=True
            if d<0 and d>d_prev and d>d_next and (d>d_prev/tol or d>d_next/tol): # Spike with coeff negatif
                spike=True
            if d> 0.035: # Case of spike with a previous or next spike
                spike=True
            if spike==True: # If spike is detected
                f_list.append(x[i]) # Add corresponding frequency
                y_list.append(averaged_spectrum_log[i]) # Add corresponding y-axis value
        return f_list,y_list
                
    def create_spectral_analysis_graph(self,period=None):
        """Plots a data graph which represents the mean of all spectrums
        annotated with spikes frequencies
        
        Parameters
        ----------
        period : float, optional
            Time between two samples
        """
        self.period=period # Update period
        # Data Graphs creation            
        self.fig=plt.figure() # Creates a data graph
        self.axes=self.fig.add_subplot(111) # Add a new graphic display
        
        # Calculates x-axis frequencies of the fourier spectrum
        nb_samples=self.averaged_spectrum.shape[0] # Number of samples
        freqs = fft.fftfreq(nb_samples,self.period)

        
        # Mean spectrum recovery
        averaged_spectrum_log=[]
        for f in self.averaged_spectrum: # For each frequency
             averaged_spectrum_log.append(log(f/self.averaged_spectrum.mean())) # Add log(module/mean)
        
        averaged_spectrum_log = np.array(averaged_spectrum_log)
        # Detection of spikes with their frequency
        min=averaged_spectrum_log.min() # Minimum value
        max=averaged_spectrum_log.max() # Maximum value
        f_list,y_list=self.detect_spikes(averaged_spectrum_log,freqs) # Detect frequencies of spikes
        for f in range(len(f_list)): # For each spike's frequency
            self.axes.plot([f_list[f],f_list[f]],[min,y_list[f]],'r') # Trace a vertical line
            plt.annotate(str(int(np.round(f_list[f]))),(np.round(f_list[f]),y_list[f]+0.1),color='r') # Write corresponding frequency
        
        # Plots data graph        
        positive_freqs_x = np.arange(nb_samples/2)
        self.axes.plot(freqs[positive_freqs_x],averaged_spectrum_log[positive_freqs_x]) # Creates the plot                 
        self.axes.set_xlabel('Frequency (Hz)') # Sets the horizontal label
        self.axes.set_ylabel('Log power') # Sets the vertical label
        self.axes.set_ylim([min,max]) # Sets the log power limit
        
        # Informations recovery
        data=os.path.split(self.spectrums_list[0])[1] # Filename
        date=data[1:7] # Date
        cdt=data[20:22] # Condition
        
        self.axes.set_title('Spectral Analysis\nDate: '+date+' ; Condition: '\
                           +cdt+' ; Region: '+self.mask_name) # Sets the data graph title
        self.axes.grid(True) # Shows the grid

    def estimate_tau_and_heartbeat_frequency(self,tau_max=6,list=None):
        """Estimate the time constant 'tau' and the heartbeat frequency over an ROI
        It uses the condition file to average a list of spectrums, using their paths

        Parameters
        ----------
        list : list, optional
            The list of images' paths which has to be averaged

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if list is not empty, -1 if list is empty
        """
        warning = 0
        
        if list == None:
            list = self.trials_list

        if list == []:
            warning = -1
            return warning  # Returns a warning list is empty
            
        # Recover spectrum list
        # Recuperation of image header and size, use the first image in the sublist
        fname = list[0].encode("utf8")
        Vol=aims.read(fname) # Opening NIFTI file
        # NIFTI header recuperation
        self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
        self.nifti_header['time_units'] = Vol.header()[ 'time_units' ]
        self.nifti_header['voxel_size'] = Vol.header()[ 'voxel_size' ]
        if self.period is None:
            self.period = self.nifti_header['voxel_size'][3]
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            
        # Recuperation of image's size
        tsize=img.shape[0]
            
        self.tau_list=[]
        self.fh_list=[] 
        self.v_list=[]          
        # Images averaging
        j=0
        while j<len(list): # For each file in the sublist
            Vol = aims.read(list[j].encode("utf8")) # Opening NIFTI file
            img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            self.averaged_img=img
            roi=self.average_region()
            roi=np.reshape(roi,(tsize))
            v=self.fit(roi,tau_max)
            tau=v[1]
            fh=v[3]
            self.tau_list.append(tau)
            self.fh_list.append(fh)
            self.v_list.append(v)
            j=j+1
             
        return warning

    def fit(self,trial,tau_max):
        """ Use Nelder-Mead simplex direct search method to try to estimate tau
        and heartbeat frequency
        """
        ## Initial parameter value
        v0 = [0., 0.9, 0., 2., 0., 0.]
        
        ## Constraints for each parameter
        bounds = [(None,None),(0.,tau_max),(None,None),(0.,4.),(None,None),(None,None)]
        
        ## Parametric function: 'v' is the parameter vector, 't' the independent variable
        dye_bleaching = lambda v, t: np.exp(-t/v[1])-1 # v[1]=tau (time constant in second)
        hearthbeat = lambda v, t: np.sin(v[3]*t+v[4]) # v[3]=fh (hearthbeat frequency in Hz), v[4]=phi (phase)
        
        self.func = lambda v, t: v[0]*dye_bleaching(v,t) + v[2]*hearthbeat(v,t) + v[5] # v[0]=a, v[2]=b and v[5]=c (Coefficients)

        ## Noisy data to fit, ie blank mean
        n = trial.shape[0] # Number of samples
        tmin = 0.
        tmax = trial.shape[0]-1
        
        t=np.linspace(tmin,tmax*self.period,n)
        trial=trial-trial[0]
        ## Error function
        e = lambda v, t, y: np.sum((self.func(v,t)-y)**2)
        
        ## Fitting
        v = fmin_l_bfgs_b(e, v0, approx_grad=True, args=(t,trial), bounds=bounds, pgtol=1e-08)[0]
        
        return v
        
    def create_fit_results_graph(self,tau_max):
        """Plots histograms of tau and fh
        """      
        # Data Graphs creation      
        self.fig=plt.figure() # Creates a data graph      
        # Median tau value
        bounds=(0,tau_max)
        new_tau_list=[]
        for t in range(len(self.tau_list)):
            if self.tau_list[t] not in bounds:
                new_tau_list.append(self.tau_list[t])
        median_tau=np.median(new_tau_list) # Get median
        # Median fh value
        bounds=(0,4)
        new_fh_list=[]
        for t in range(len(self.fh_list)):
            if self.fh_list[t] not in bounds:
                new_fh_list.append(self.fh_list[t])
        median_fh=np.median(new_fh_list) # Get median
        
        # Two best trials recovery
        i=0
        dist_list=[]
        for i in range(len(self.v_list)):
            dist_list.append(abs(median_tau-self.tau_list[i])+abs(median_fh-self.fh_list[i]))
        
        min1=np.array(dist_list).min()
        i1=dist_list.index(min1)
        cut_dist_list=dist_list
        del cut_dist_list[i1]
        min2=np.array(cut_dist_list).min()
        i2=dist_list.index(min2)
                    
        self.axes=self.fig.add_subplot(411) # Add a new graphic display                  
        Vol = aims.read(self.trials_list[i1].encode("utf8")) # Opening NIFTI file
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
        n = img.shape[0] # Number of samples
        tmin = 0.
        tmax = img.shape[0]-1      
        t=np.linspace(tmin,tmax*self.period,n)
                    
        self.averaged_img=img
        roi=self.average_region()
        trial=np.reshape(roi,(n))
        trial=trial-trial[0]
        self.axes.plot(t,self.func(self.v_list[i1],t),'r',t,trial,'b')
        
        data=os.path.split(self.trials_list[0])[1]
        date=data[1:7] # Date
        self.axes.set_title('Date: '+date+' , ROI: ('+str(self.xmin)+','+str(self.ymin)\
                           +') to ('+str(self.xmax)+','+str(self.ymax)+')') # Sets the figure title

        self.axes=self.fig.add_subplot(412) # Add a new graphic display
        Vol = aims.read(self.trials_list[i2].encode("utf8")) # Opening NIFTI file
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
        self.averaged_img=img
        roi=self.average_region()
        trial=np.reshape(roi,(n))
        trial=trial-trial[0]
        self.axes.plot(t,self.func(self.v_list[i2],t),'r',t,trial,'b')

                    
        self.axes = self.fig.add_subplot(413)
        # Histogram of tau values
        n_tau,bins_tau,patches_tau=self.axes.hist(np.array(new_tau_list),bins=100)
        
        # Figure of tau
        mini=np.array(new_tau_list).min()
        maxi=np.array(new_tau_list).max() 
        nmax=n_tau.max()

        plt.annotate('Tau = '+str(round(median_tau,2))[:4],((maxi+mini)/2.,nmax*(5./6)),color='r')
        self.axes.set_xlabel('Tau value') # Sets the horizontal label
        self.axes.set_xlim([mini,maxi]) # Sets horizontal limits


        self.axes.set_ylabel('Trials number') # Sets vertical title
    
        self.axes.grid(True) # Shows the grid
        
        self.axes = self.fig.add_subplot(414)

        # Histogram of fh values
        n_fh,bins_fh,patches_fh=self.axes.hist(np.array(new_fh_list),bins=100)
        nmax=n_fh.max()
        # Figure of fh
        mini=np.array(new_fh_list).min()
        maxi=np.array(new_fh_list).max()        
        self.axes.set_xlabel('Fh value') # Sets the horizontal label
        self.axes.set_xlim([mini,maxi]) # Sets horizontal limits
        self.axes.set_ylabel('Trials number') # Sets vertical title

        plt.annotate('Fh = '+str(round(median_fh,2))[:4],((maxi+mini)/2.,nmax*(5./6)),color='r')        
        self.axes.set_title('\n') # Sets the figure title
        self.axes.grid(True) # Shows the grid
        
    def create_and_average_d_values(self,residuals_list=None,betas_list=None):
        """Creates and averages a list of residuals over an ROI
        It uses the condition file to average a list of spectrums, using their paths

        Parameters
        ----------
        residuals_list : list, optional
            The list of residuals' paths which has to be averaged
        betas_list : list, optional
            The list of betas' paths which has to be averaged

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if list is not empty, -1 if list is empty
        """
        warning = 0
        
        if residuals_list == None:
            residuals_list = self.residuals_list

        if betas_list == None:
            betas_list = self.betas_list
            
        if residuals_list == [] or betas_list == []:
            warning = -1
            return warning  # Returns a warning list is empty
            
        # Recover residuals list
        # Recuperation of image header and size, use the first image in the sublist
        fname = residuals_list[0].encode("utf8")
        Vol=aims.read(fname) # Opening NIFTI file
        # NIFTI header recuperation
        self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
        self.nifti_header['time_units'] = Vol.header()[ 'time_units' ]
        self.nifti_header['voxel_size'] = Vol.header()[ 'voxel_size' ]
        if self.period is None:
            self.period = self.nifti_header['voxel_size'][3]
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            
        # Recuperation of image's size
        tsize=img.shape[0]
        zsize=img.shape[1]
        ysize=img.shape[2]
        xsize=img.shape[3]            
            
        # Images averaging
        j=0
        self.d_values=np.zeros([len(residuals_list)]) # Durbin-Watson test value list initialization
        while j<len(residuals_list): # For each file in the sublist  
        
            # Recovery of betas
            Vol = aims.read(betas_list[j].encode("utf8")) # Opening NIFTI file
            img_betas=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            betas=np.reshape(img_betas,(img_betas.shape[0],zsize*ysize*xsize))
            
            # Recovery of residuals
            Vol = aims.read(residuals_list[j].encode("utf8")) # Opening NIFTI file
            img_res=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            signal=np.reshape(img_res,(tsize,zsize*ysize*xsize))
            reconstr=signal/betas[0].mean()
            signal=reconstr.copy() # Copy of reconstr (see numpy help for the use of copy in matrix)

            signal=np.reshape(signal,(tsize,zsize,ysize,xsize)) # Reshape from a time + 1D image to a time + 3D image (time,x,y,z)
        
            signal=(np.isnan(signal)).choose(signal,0).copy() # Remplacement of NaN by 0

            self.averaged_img=(np.isinf(signal)).choose(signal,0).copy() # Remplacement of Inf by 0
            
            residuals=self.average_region()
            
            self.d_values[j]=self.durbin_watson_test(residuals) # Add value
            j=j+1
            
        return warning
        
    def durbin_watson_test(self,trial):
        """ Apply Durbin-Watson test
        
        Parameters
        ----------
        trial : numpy array
            Trial data on which will be apply Durbin-Watson test
            
        Returns
        -------
        d : float
            D-value result of the Durbin-Watson test 
        """
        # Durbin-Watson test
        numerator=0 # Sum of squared deviation
        for i in range(1,len(trial)):
            numerator+=(trial[i]-trial[i-1])**2
        denominator=0 # Sum of squared values
        for i in range(len(trial)):
            denominator+=trial[i]**2
        d=numerator/denominator
        return d
        
    def create_durbin_watson_results_graph(self):
        """Plots the durbin watson results in a histogram
        """
        # Data Graphs creation            
        self.fig=plt.figure() # Creates a data graph
        self.axes=self.fig.add_subplot(111) # Add a new graphic display
        
        # Median d value
        median_d=np.median(self.d_values) # Get median      
        
        # Histogram of d value
        n,bins,patches=self.axes.hist(self.d_values,bins=100)
        
        # Figure
        mini=self.d_values.min()
        maxi=self.d_values.max()
        nmax=n.max()
        self.axes.plot([1,1],[0,n.max()],'r',[2,2],[0,n.max()],'r')        
        self.axes.set_xlabel('D-value') # Sets the horizontal label
        self.axes.set_xlim([mini,maxi]) # Sets horizontal limits
        self.axes.set_ylabel('Trials number') # Sets vertical title
        plt.annotate('D-value median = '+str(round(median_d,2)),((maxi+mini)/2.,nmax*(5./6)),color='r')
        
        data=os.path.split(self.residuals_list[0])[1]
        date=data[1:7] # Date
        self.axes.set_title('Durbin-Watson Test on residuals\nDate: '+date\
                           +' , Region: '+self.mask_name) # Sets the figure title
        self.axes.grid(True) # Shows the grid
        
    def average_trials_on_rois(self,rois_list,trials_list=None):
        """Creates groups with the differents ROIs
        Uses the condition file to average a list of trials, using their paths

        Parameters
        ----------
        rois_list : list
            The list of the different ROIs which can be a tuple of two int tuple
            or a binary mask path
        trials_list : list, optional
            The list of trials' paths which has to be averaged
            
        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if list is not empty, -1 if list is empty
        """
        warning = 0
        
        if trials_list == None:
            trials_list = self.trials_list
            
        if trials_list == []:
            warning = -1
            return warning  # Returns a warning list is empty
            
        # Recover rois list        
        self.nb_rois=len(rois_list) # Number of different ROIs
        # Recuperation of image header and size, use the first image in the sublist
        fname = trials_list[0].encode("utf8")
        Vol=aims.read(fname) # Opening NIFTI file
        # NIFTI header recuperation
        self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
        self.nifti_header['time_units'] = Vol.header()[ 'time_units' ]
        self.nifti_header['voxel_size'] = Vol.header()[ 'voxel_size' ]
        if self.period is None:
            self.period = self.nifti_header['voxel_size'][3]
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
        self.load_averaged_img(path=trials_list[0]) # Loads an existing image          
            
        # Images averaging
        j=0
        self.averaged_img_list=[]
        while j<len(trials_list): # For each trial in the list  

            # Recovery of trial
            Vol = aims.read(trials_list[j].encode("utf8")) # Opening NIFTI file
            img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            self.averaged_img=img
            for roi in rois_list:
                if type(roi)==str:
                    self.load_mask(path=roi)
                else:
                    # typ should be tuple or list
                    self.create_rectangle_mask(roi[0],roi[1])# Creates a rectangle mask which defines the region to average
                averaged_trial=self.average_region()
                self.averaged_img_list.append(averaged_trial) # 4-D array of the signal

            j=j+1
            
        return warning   

    def group_trials_by_roi(self,rois_list):
        """Group trials by Region Of Interest and update legend names
        
        Parameters
        ----------
        rois_list : list
            The list of the different ROIs which can be a tuple of two int tuple
            or a binary mask path
        """
        # Sort by ROI        
        self.leg_names=[] 
        self.groups_list=[] # Trials sorted by position list
        for roi in range(self.nb_rois): # For each ROI
            self.groups_list.append(self.extract_mean_group(roi))
            if type(rois_list[roi])==tuple: # If ROI is a corner tuple
                self.leg_names.append(rois_list[roi])
            if type(rois_list[roi])==str: # If ROI is binary mask path
                self.leg_names.append(os.path.basename(rois_list[roi])[:-4])
       
    def extract_mean_group(self,id_roi):
        """Extract mean of trials grouped by an unique ROI
        
        Parameters
        ----------
        id_roi : int
            ROI position in the ROIs list
            
        Returns
        -------
        mean_group : numpy 1-D array
            Values of the mean of trials grouped by an unique ROI
        """
        mean_group=np.zeros([self.averaged_img.shape[0],1])
        for i in range(len(self.averaged_img_list)/self.nb_rois):
            mean_group+=self.averaged_img_list[i*self.nb_rois+id_roi]
        mean_group=mean_group/(len(self.averaged_img_list)/self.nb_rois)
        mean_group=np.reshape(mean_group,(self.averaged_img.shape[0]))
        return mean_group
             
    def trace_comparison_of_rois_graph(self):
        """Plots the comparison of rois graph
        """
        # Data Graphs creation            
        self.fig=plt.figure() # Creates a data graph
        self.axes=self.fig.add_subplot(111) # Add a new graphic display
        
        date=os.path.split(self.trials_list[0])[1][1:7] # Session date    
        title = 'Comparison of ROIs'
        x=np.linspace(-self.start*self.period*1000,(self.averaged_img.shape[0]-self.start)*self.period*1000,110)
        for i in range(len(self.groups_list)):
            if self.groups_list[i] != []:
                self.axes.plot(x,self.groups_list[i])
        
        self.axes.set_xlabel('Time (ms)') # Sets horizontal title
        self.axes.set_xlim([-self.start*self.period*1000,(self.averaged_img.shape[0]-self.start)*self.period*1000]) # Sets horizontal mask
        # The following line will be uncommented with BrainVISA 4.4 with a more recent version of matplotlib
        #self.axes.set_ylabel(r'$\Delta$'+'F/F') # Sets vertical title
        self.axes.set_ylabel('Normalized signal') # Sets vertical title
        self.axes.set_title(title+'\nDate : '+date) # Sets title

        box=self.axes.get_position()
        if len(self.leg_names)<3:
            ncol=len(self.leg_names)
        else:
            ncol=3
        self.axes.set_position([box.x0,box.y0+box.height*0.2,box.width,box.height*0.8])
        leg=self.axes.legend(self.leg_names,'upper center'\
                            ,bbox_to_anchor=(0.5, -0.1)\
                            ,fancybox=True\
                            ,shadow=True\
                            ,ncol=ncol)
        for t in leg.get_texts():
            t.set_fontsize('small') # Sets the size of text legend
    
        for l in leg.get_lines():
            l.set_linewidth(1.5) # Sets the width of line signal legend
            
        self.axes.hold(False)
        self.axes.grid(True) # Shows the grid

    def delete_trials(self,trials):
        """ Return trials list without higher and lower trial
        
        Parameters
        ----------
        trials : str list
            Trials path list
            
        Returns
        -------
        selection : str list
            Trials path list which not include higher and lower trial
        """
        # Treatment to delete higher and lower trial           
        maxi=0 # Higher value
        mini=0 # Lower value
        high_pos=0
        low_pos=0
        for i in range(len(trials)): # For each trial
            max_loc=np.array(trials[i]).max() # Higher value of the trial
            min_loc=np.array(trials[i]).min() # Lower value of the trial
            if max_loc>maxi: # If better value
                maxi=max_loc # Change higher value
                high_pos=i # Save trial idendity
            if min_loc<mini: # If worse value
                mini=min_loc # Change lower value
                low_pos=i # Save trial identity
        
        #Determine the new trials selection
        if low_pos==high_pos: # If higher and lower signal are the same signal
            selection=trials[:high_pos]+trials[high_pos+1:] # Delete the single trial
        elif low_pos<high_pos: # Delete lower then higher trial
            selection=trials[:low_pos]+trials[low_pos+1:high_pos]+trials[high_pos+1:]
        else: # Delete higher then lower trial
            selection=trials[:high_pos]+trials[high_pos+1:low_pos]+trials[low_pos+1:]
            
        return selection
              
    def trace_trials_variability_graph(self):
        """Plots the trials variability
        """
        # Data graph creation        
        self.fig = plt.figure() # Creates a data graph
        self.axes = self.fig.add_subplot(111) # Add a new graphic display
        self.axes.hold(True) # Plot all trials on the same data graph      
        
        nb_samples=self.averaged_img.shape[0] # Number of samples
        x=np.linspace(-self.start*self.period*1000\
                    ,(nb_samples-self.start)*self.period*1000\
                    ,nb_samples) # Time
                    
        selection=self.delete_trials(self.averaged_img_list)  # Trials selection
            
        for trial in selection: # For each averaged trial
            self.axes.plot(x,trial,'k') # Plots trial in black
        
        mean_trials=np.zeros([self.averaged_img.shape[0],1])
        for t in range(len(selection)):
            mean_trials+=selection[t]
        mean_trials=mean_trials/len(selection)# Mean trials recovery
        mean_trials=np.reshape(mean_trials,(self.averaged_img.shape[0]))
        
        self.axes.plot(x,mean_trials,self.color,linewidth=2) # Plots the mean
        
        self.axes.set_xlabel('Time (ms)') # Sets horizontal title
        self.axes.set_xlim([-self.start*self.period*1000,(nb_samples-self.start)*self.period*1000]) # Sets horizontal limits
        # The following line will be uncommented with BrainVISA 4.4 with a more recent version of matplotlib
        #self.axes.set_ylabel(r'$\Delta$'+'F/F') # Sets vertical title
        self.axes.set_ylabel('Normalized signal') # Sets vertical title
       
        data=os.path.split(self.trials_list[0])[1] # Informations recovery
        date=data[1:7] # Session date
        self.axes.set_title('Visualization of Trials Variability\nDate: '+date+\
                            ' ; Region: '+self.mask_name) # Sets the data graph title
        self.axes.hold(False)
        self.axes.grid(True) # Shows the grid
      
    def average_trials_of_each_analysis(self,analysis_list=None):
        """Creates groups with the differents analysis
        It uses the condition file to average a list of trials, using their paths

        Parameters
        ----------
        analysis_list : list, optional
            The list of images' paths which has to be averaged

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if list is not empty, -1 if list is empty
        """
        warning = 0
        
        if analysis_list == None:
            analysis_list = self.analysis_list
            
        if analysis_list == []:
            warning = -1
            return warning  # Returns a warning list is empty
            
        # Recover rois list
        self.nb_groups=len(self.analysis_list) # Number of different groups
        
        # Recuperation of image header and size, use the first image in the sublist
        fname = analysis_list[0][0].encode("utf8")
        Vol=aims.read(fname) # Opening NIFTI file
        # NIFTI header recuperation
        self.nifti_header['xyz_units'] = Vol.header()['xyz_units']
        self.nifti_header['time_units'] = Vol.header()[ 'time_units' ]
        self.nifti_header['voxel_size'] = Vol.header()[ 'voxel_size' ]
        if self.period is None:
            self.period = self.nifti_header['voxel_size'][3]
        img=np.transpose(np.array( Vol,copy=True )) # Image recuperation         
            
        # Images averaging
        j=0
        paths_list=[]
        for i in range(self.nb_groups): # For each list of paths list
            paths_list+=self.analysis_list[i] # Concatenate paths lists
            
        self.averaged_img_list=[]
        while j<len(paths_list): # For each trial in the list  

            # Recovery of trial
            Vol = aims.read(paths_list[j].encode("utf8")) # Opening NIFTI file
            img=np.transpose(np.array( Vol,copy=True )) # Image recuperation
            self.averaged_img=img
            averaged_trial=self.average_region()
            self.averaged_img_list.append(averaged_trial) # 4-D array of the signal
            j=j+1
            
        return warning   

    def group_trials_by_analysis(self):
        """ Group all averaged trials by analysis and average them
        """
        self.mean_of_each_analysis_list=[]
        current_path=0
        for i in range(self.nb_groups):
            groups_list=self.averaged_img_list[current_path:current_path+len(self.analysis_list[i])]      
            current_path+=len(self.analysis_list[i])
            mean_group=np.zeros([self.averaged_img.shape[0],1])
            for i in range(len(groups_list)):
                mean_group+=groups_list[i]
            mean_group=mean_group/len(groups_list)
            self.mean_of_each_analysis_list.append(np.reshape(mean_group,(self.averaged_img.shape[0])))           

    def trace_comparison_of_analysis_graph(self):
        """Plots the comparison of rois graph
        """
        # Data Graphs creation            
        self.fig=plt.figure() # Creates a data graph
        self.axes=self.fig.add_subplot(111) # Add a new graphic display
        
        date=os.path.split(self.analysis_list[0][0])[1][1:7] # Session date    
        title = 'Comparison of Analysis\nDate : '+str(date)+' ; Region: '+self.mask_name # Sets the data graph title
        print self.period
        print -self.start*self.period*1000
        print (self.averaged_img.shape[0]-self.start)*self.period*1000
        print self.averaged_img.shape[0]
        x=np.linspace(-self.start*self.period*1000,(self.averaged_img.shape[0]-self.start)*self.period*1000,self.averaged_img.shape[0])
        for i in range(len(self.mean_of_each_analysis_list)):
            self.axes.plot(x,self.mean_of_each_analysis_list[i])
        
        self.axes.set_xlabel('Time (ms)') # Sets horizontal title
        self.axes.set_xlim([-self.start*self.period*1000,(self.averaged_img.shape[0]-self.start)*self.period*1000])
        # The following line will be uncommented with BrainVISA 4.4 with a more recent version of matplotlib
        #self.axes.set_ylabel(r'$\Delta$'+'F/F') # Sets vertical title
        self.axes.set_ylabel('Normalized signal') # Sets vertical title
        self.axes.set_title(title) # Sets title
        
        box=self.axes.get_position()
        if len(self.leg_names)<3:
            ncol=len(self.leg_names)
        else:
            ncol=3
        self.axes.set_position([box.x0,box.y0+box.height*0.2,box.width,box.height*0.8])
        leg=self.axes.legend(self.leg_names,'upper center'\
                            ,bbox_to_anchor=(0.5, -0.1)\
                            ,fancybox=True\
                            ,shadow=True\
                            ,ncol=ncol)
        for t in leg.get_texts():
            t.set_fontsize('small') # Sets the size of text legend
    
        for l in leg.get_lines():
            l.set_linewidth(1.5) # Sets the width of line signal legend
            
        self.axes.hold(False)
        self.axes.grid(True) # Shows the grid      
        
    def save_data_graph(self,path_data_graph):
        """Saves the data graph of spectral analysis as an PNG image
        
        Parameters
        ----------
        path_data_graph : str
            Path of the data graph which will be saved
        """
        directory=os.path.split(path_data_graph)[0]
        if os.path.exists(directory)==False:
            os.mkdir(os.path.split(directory)[0])
            os.mkdir(directory)
        self.fig.savefig(path_data_graph,format='png')
                 
    def save_model_definition( self,analysis, path=None ):
        """Saves X.txt in session_level (in vsdi_data)

        Parameters
        ----------
        analysis : str
            The name of the analysis directory
        path : str, optional
            The path where the model definition will be saved

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if the path really exist, -1 if the path doesn't exist
        pathGLM : str
            The path where the model definition is saved

        Raises
        ------
        ValueError
            If the path does not exist
        """
        warning=0

        self.path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis',analysis)

        if self.database_mode==False: # If not using the database mode
            if path == None:# If no path defined
                raise ValueError('Path argument not defined') # Raises an exception
            else:
                pathGLM=path # Recuperation of path

        else:
            pathGLM=os.path.join(self.path,'glm.txt') # pathGLM creation

            if path != None:
                if path != pathGLM:
                    print path
                    print pathGLM
                    warning=-1 # Returns a warning if the path does not correspond to the path created by the database mode
                pathGLM=path

        if os.path.lexists(os.path.split(pathGLM)[0]) == False and os.path.split(pathGLM)[0] != '': # If pathGLM does not exist
            os.makedirs(os.path.split(pathGLM)[0]) # Creation of path

        np.savetxt(pathGLM,self.X) # Save of model definition in a txt file

        return (warning,pathGLM)

    def save_model_parameters( self, analysis, path=None ):
        """Saves param.npz in session_level (in vsdi_data)

        Parameters
        ----------
        analysis : str
            The name of the analysis directory
        path : str, optional
            The path where to save the model parmeters
            

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if the path really exist, -1 if the path doesn't exist
        path : str
            The path of the model parmeters

        Raises
        ------
        ValueError
            If the path does not exist
        """
        warning=0

        self.path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis',analysis)

        if self.database_mode==False: # If not using the database mode
            if path == None: # If no path defined
                raise ValueError('Path argument not defined') # Raises an exception
            else:
                pathParam=path # Recuperation of path
        else:
            pathParam=os.path.join(self.path,'param.npz')
            if path != None:
                if path != pathParam:
                    print path
                    print pathParam
                    warning=-1 # Returns a warning if the path does not correspond to the path created by the database mode
                pathParam=path

        if os.path.lexists(os.path.split(pathParam)[0]) == False and os.path.split(pathParam)[0] != '': # If pathParam does not exist
            os.makedirs(os.path.split(pathParam)[0]) # Creation of path

        f=open(pathParam.encode("utf8"),'w') # Save of model definition in a npz file
        pickle.dump(self.param,f)
        f.close()

        return (warning,pathParam)

    def save_averaged(self, analysis, format=None):
        """Saves the averaged images

        Parameters
        ----------
        analysis : str
            The name of the analysis directory
        format : {'.nii','.nii.gz'}, optional
            Saving format of images. It can be NIFTI-1 Image ('.nii') or gzip compressed NIFTI-1 Image ('.nii.gz')

        Returns
        -------
        path : str
            The path of the averaged image
        """
        # Setting format. If format is not defined, using the last format definition
        if format != None:
            self.__setformat(format)

        self.path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis',analysis) # Path creation

        if analysis[0:9]=='glm_based':
            analysis = analysis + '_denoised'
        elif analysis[0:11]=='blank_based':
            analysis = analysis + '_bksd'
        elif analysis[0:3]=='raw':
            analysis = analysis

        filename_list=[]        
        for cond_list in self.cond_group_list: # For each condition list
            filename = 's' + self.session[8:16] + '_' + analysis # Filename creation
            for cond in cond_list: # For each condition
                filename = filename + '_c' + str(cond).zfill(3) # Adding condition's name
            filename = filename + '_mean' + self.format # Adding the suffix '_mean' and file extension
            filename_list.append(filename) # Adding filename to filename list
        for i in range(len(filename_list)):   
            self.__save_nifti( os.path.join(self.path,filename_list[i]),self.averaged_img_list[i], self.nifti_header ) # Save of NIFTI-1 image

        return self.path

    def save_mask( self,path=None,format=None ):
        """Saves the mask in vsdi_data

        Parameters
        ----------
        path : str, optional
            The path where to save the mask
        format : {'.nii','.nii.gz'}, optional
            Saving format of images. It can be NIFTI-1 Image ('.nii') or gzip compressed NIFTI-1 Image ('.nii.gz')

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if the mask really exist, -1 if the mask doesn't exist
        pathMask : str,
            The path where the mask is saved

        Raises
        ------
        ValueError
            If the path does not exist
        """
        warning=0

        # Setting format. If format is not defined, using the last format definition
        if format != None:
            self.__setformat(format)

        self.path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis','mask') # Path creation

        if self.database_mode==False: # If not using the database mode
            if path == None:
                raise ValueError('Path argument not defined') # Raises an exception
            else:
                pathMask=path # Path recuperation
        else:
            pathMask=os.path.join(self.path,self.mask_name+self.format) # Mask's path creation
            if path != None:
                if path != pathMask:
                    print path
                    print pathMask
                    warning=-1 # Returns a warning if the path does not correspond to the path created by the database mode
                pathMask=path

        if os.path.lexists(os.path.split(pathMask)[0]) == False and os.path.split(pathMask)[0] != '': # If pathMask does not exist
            os.makedirs(os.path.split(pathMask)[0])  # Creation of path
        nifti_header={'xyz_units':2, 'time_units':8, 'voxel_size':[1,1,1,1]} # Setting NIFTI header
        self.__save_nifti( pathMask,self.mask,nifti_header ) # Save of NIFTI-1 image
        
        return (warning,pathMask)

    def save_averaged_region( self, analysis,path=None,format=None ):
        """Saves the averaged time serie

        Parameters
        ----------
        analysis : str
            The name of the analysis directory
        path : str
            The path where to save the averaged region
        format : {'.nii','.nii.gz'}, optional
            Saving format of images. It can be NIFTI-1 Image ('.nii') or gzip compressed NIFTI-1 Image ('.nii.gz')

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if the path really exist, -1 if the path doesn't exist
        pathAverRegion : str
            The path where the averaged region is saved

        Raises
        ------
        ValueError
            If the path does not exist
        """
        warning=0

        self.path=os.path.join(self.database,self.protocol,self.subject,self.session,'oisession_analysis',analysis)

        if self.database_mode==False: # If not using the database mode
            if path == None:
                raise ValueError('Path argument not defined') # Raises an exception
            else:
                pathAverRegion=path # Path recuperation
        else:
            pathAverRegion=os.path.join(self.path,os.path.splitext(os.path.splitext(self.filename)[0])[0]+'_'+self.mask_name+'.txt') # Averaged region's path creation
            if path != None:
                if path != pathAverRegion:
                    print path
                    print pathAverRegion
                    warning=-1 # Returns a warning if the path does not correspond to the path created by the database mode
                pathAverRegion=path

        if os.path.lexists(os.path.split(pathAverRegion)[0]) == False and os.path.split(pathAverRegion)[0] != '': # If pathAverRegion does not exist
            os.makedirs(os.path.split(pathAverRegion)[0])

        np.savetxt(pathAverRegion,self.averaged_region) # Saving the averaged time serie in a txt file
        
        return (warning,pathAverRegion)

    def update_database( self,path=None ):
        """Register created files in brainvisa's database

        Parameters
        ----------
        path : str, optional
            The path of the directory containing datas to update

        Returns
        -------
        warning : {0,-1}
            Takes as value : 0 if the path really exist, -1 if the path doesn't exist
        """
        warning = 0
        print 'db = ', self.database
        if path == None: # If no path defined, updating the whole database
            db=databases.database( self.database ) # Loading database
            db.update( self.path )

        elif path != None:
            if os.path.lexists( path ): # If path exists
                db=databases.database( self.database ) # Loading database
                db.update( path ) # Updating the defined directory
            else:
                warning = -1  # Returns a warning if path does not exists
                return warning
        else:
            warning = -1
            return warning

        return warning
        
    def __setformat(self,format):
        """Set the NIFTI format (NIFTI-1 Image or gzip compressed NIFTI-1 Image)

        Parameters
        ----------
        format : {'.nii','.nii.gz'}
            Saving format of images. It can be NIFTI-1 Image ('.nii') or gzip compressed NIFTI-1 Image ('.nii.gz')

        Raises
        ------
        ValueError
            If extension of files are differents of '.nii', '.nii.gz'
        """
        if format=='.nii' or format =='.nii.gz': # If format is NIFTI-1 or gzip compressed NIFTI-1
            self.format=format # Sets format
        else:
            raise ValueError("File extension not supported") # Raises an exception

    def __save_nifti( self,path,image,header ):
        """Save of NIFTI-1 image

        Parameters
        ----------
        path : str
            The saving path of the image
        image : numpy array
            The image to save
        header : dict
            The NIFTI header
        """
        if os.path.lexists(os.path.split(path)[0]) == False and os.path.split(path)[0] != '': # If path does not exist
            os.makedirs(os.path.split(path)[0]) # Directory creation

        image=np.array( np.transpose(image),order='F',dtype=np.float32 ) # Adaptation of array-type for a use in pyNIFTI
        Vol = aims.Volume_FLOAT( image ) # Volume creation
        # Setting headers
        Vol.header()[ 'xyz_units' ] = header[ 'xyz_units' ]
        Vol.header()[ 'time_units' ] = header[ 'time_units' ]
        Vol.header()[ 'voxel_size' ] = header[ 'voxel_size' ]
        aims.write( Vol, path.encode("utf8") ) # Saving data
